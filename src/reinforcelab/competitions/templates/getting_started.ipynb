{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"logo.png\" alt=\"Competition Logo\" width=\"200\"/>\n",
        "\n",
        "# ___TITLE___\n",
        "\n",
        "Welcome to the competition! This notebook will help you get started. You'll learn how to:\n",
        "1. Initialize and interact with the environment\n",
        "2. Create and use a random agent as a baseline\n",
        "3. Run local evaluation using `run_local.py`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup\n",
        "\n",
        "First, let's install the required dependencies:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -q -r requirements.txt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Initialize the Environment\n",
        "\n",
        "Let's create and explore the environment:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "\n",
        "# Environment ID for this competition\n",
        "ENV_ID = \"___ENV_ID___\"\n",
        "\n",
        "# Create the environment\n",
        "env = gym.make(ENV_ID)\n",
        "\n",
        "print(f\"Environment: {ENV_ID}\")\n",
        "print(f\"Observation Space: {env.observation_space}\")\n",
        "print(f\"Action Space: {env.action_space}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Render the Initial State\n",
        "\n",
        "Let's reset the environment and try to render the initial state:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Reset the environment to get the initial observation\n",
        "observation, info = env.reset()\n",
        "\n",
        "print(f\"Initial Observation: {observation}\")\n",
        "print(f\"Observation Shape: {observation.shape if hasattr(observation, 'shape') else type(observation)}\")\n",
        "\n",
        "# Try to render the environment (may not work in all environments)\n",
        "try:\n",
        "    env_render = gym.make(ENV_ID, render_mode=\"rgb_array\")\n",
        "    env_render.reset()\n",
        "    frame = env_render.render()\n",
        "    \n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.imshow(frame)\n",
        "    plt.title(\"Initial State\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "    env_render.close()\n",
        "except Exception as e:\n",
        "    print(f\"Rendering not available: {e}\")\n",
        "    print(\"This is normal for some environments. You can still train and evaluate agents.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Random Agent Baseline\n",
        "\n",
        "Let's use the random agent as a baseline. This agent takes random actions and serves as a starting point:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import and create the random agent\n",
        "from random_agent import Agent\n",
        "\n",
        "agent = Agent(env)\n",
        "print(f\"Agent created: {type(agent).__name__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Take a Random Action and Render\n",
        "\n",
        "Let's use the agent to take a random action and see what happens:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reset the environment\n",
        "observation, info = env.reset()\n",
        "\n",
        "# Get a random action from the agent\n",
        "action = agent.act(observation)\n",
        "print(f\"Action taken: {action}\")\n",
        "\n",
        "# Take a step in the environment\n",
        "next_observation, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "print(f\"\\nStep Result:\")\n",
        "print(f\"  Reward: {reward}\")\n",
        "print(f\"  Terminated: {terminated}\")\n",
        "print(f\"  Truncated: {truncated}\")\n",
        "print(f\"  Next Observation: {next_observation}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Try to render the state after taking an action\n",
        "try:\n",
        "    env_render = gym.make(ENV_ID, render_mode=\"rgb_array\")\n",
        "    env_render.reset()\n",
        "    env_render.step(action)\n",
        "    frame = env_render.render()\n",
        "    \n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.imshow(frame)\n",
        "    plt.title(f\"State after action: {action}\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "    env_render.close()\n",
        "except Exception as e:\n",
        "    print(f\"Rendering not available: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Run a Full Episode\n",
        "\n",
        "Let's run a complete episode and track the total reward:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_episode(env, agent, max_steps=1000):\n",
        "    \"\"\"Run a single episode and return the total reward.\"\"\"\n",
        "    observation, _ = env.reset()\n",
        "    total_reward = 0\n",
        "    steps = 0\n",
        "    \n",
        "    done = False\n",
        "    while not done and steps < max_steps:\n",
        "        action = agent.act(observation)\n",
        "        observation, reward, terminated, truncated, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "        done = terminated or truncated\n",
        "        steps += 1\n",
        "    \n",
        "    return total_reward, steps\n",
        "\n",
        "# Run multiple episodes to get average performance\n",
        "num_episodes = 10\n",
        "rewards = []\n",
        "\n",
        "for ep in range(num_episodes):\n",
        "    reward, steps = run_episode(env, agent)\n",
        "    rewards.append(reward)\n",
        "    print(f\"Episode {ep + 1}: Reward = {reward:.2f}, Steps = {steps}\")\n",
        "\n",
        "print(f\"\\n--- Random Agent Baseline ---\")\n",
        "print(f\"Average Reward: {np.mean(rewards):.2f}\")\n",
        "print(f\"Std Deviation: {np.std(rewards):.2f}\")\n",
        "print(f\"Min/Max: {np.min(rewards):.2f} / {np.max(rewards):.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Using run_local.py for Evaluation\n",
        "\n",
        "The `run_local.py` script allows you to test your agent implementation locally before submitting.\n",
        "\n",
        "To use it, you need to:\n",
        "1. Create an `agent.py` file with your `Agent` class\n",
        "2. Run `python run_local.py` from the command line\n",
        "\n",
        "Let's test this with the random agent:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Copy random_agent.py to agent.py so run_local.py can find it\n",
        "import shutil\n",
        "shutil.copy('random_agent.py', 'agent.py')\n",
        "print(\"Copied random_agent.py to agent.py\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run the local evaluation script\n",
        "import subprocess\n",
        "result = subprocess.run(['python', 'run_local.py'], capture_output=True, text=True)\n",
        "print(result.stdout)\n",
        "if result.stderr:\n",
        "    print(\"Errors:\", result.stderr)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Next Steps\n",
        "\n",
        "Now that you understand the basics, here's how to create your own agent:\n",
        "\n",
        "1. **Start from the template**: Copy `random_agent.py` and rename it to `agent.py`\n",
        "\n",
        "2. **Implement your agent**: Modify the `Agent` class to implement your RL algorithm:\n",
        "   - `__init__(self, env)`: Initialize your model/network\n",
        "   - `act(self, observation)`: Return an action given the current observation\n",
        "   - `train(self)`: Train your agent on the environment\n",
        "   - `load(self, path)`: Load saved model weights\n",
        "   - `save(self, path)`: Save your trained model\n",
        "\n",
        "3. **Test locally**: Use `run_local.py` to test your implementation\n",
        "\n",
        "4. **Build a submission**: Use `build_submission.py` to create the zip file (see below)\n",
        "\n",
        "5. **Submit**: Upload the zip to the competition page\n",
        "\n",
        "Good luck! ðŸš€\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Building a Submission\n",
        "\n",
        "The `build_submission.py` script creates a zip file ready for submission. It zips the **contents** of a folder (not the folder itself), which is what Codabench expects.\n",
        "\n",
        "**From the command line** (run from the starting kit directory):\n",
        "\n",
        "```bash\n",
        "# Default: zips sample_submission/ â†’ sample_submission.zip\n",
        "python build_submission.py\n",
        "\n",
        "# Custom input and output\n",
        "python build_submission.py -i sample_submission -o my_submission.zip\n",
        "```\n",
        "\n",
        "**Options:**\n",
        "- `-i`, `--input`: Folder to zip (default: `sample_submission`)\n",
        "- `-o`, `--output`: Output zip path (default: `sample_submission.zip`)\n",
        "\n",
        "Place your `agent.py`, any model files (`model.pt`, etc.) and extra artifacts needed (e.g. `requirements.txt`) in the folder you zip, then upload the resulting zip to the competition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clean up\n",
        "env.close()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
